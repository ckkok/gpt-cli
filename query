#!/usr/bin/env python
from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS
import os
import sys
import xmltodict
import shutil
import requests
from bs4 import BeautifulSoup
from glob import glob
import easyocr
import cv2
import os
import pickle
import pathlib
from dotenv import load_dotenv
load_dotenv()

STORE_FILE = 'store.faiss'
shutil.rmtree(STORE_FILE)

text_splitter = CharacterTextSplitter(
        separator = "\n",
        chunk_size = 1000,
        chunk_overlap = 200,
        length_function = len,
)

def save_store(store):
    store.save_local(STORE_FILE)

def get_store(file, texts):
    if not os.path.exists(file):
        if texts is None:
            return None
        store = FAISS.from_texts(texts, OpenAIEmbeddings())
        save_store(store)
        return store
    return FAISS.load_local(STORE_FILE, OpenAIEmbeddings())

def get_text_from_file(file):
    with open(file, 'r') as f:
        return f.read()

def get_text_from_pdf(file):
    reader = PdfReader(file)
    raw_text = ''
    for i, page in enumerate(reader.pages):
        text = page.extract_text()
        if text:
            raw_text += text
    return raw_text

def get_text_from_image(file):
    img = cv2.imread(file)
    reader = easyocr.Reader(['en'])
    result = reader.readtext(img, detail = 0, paragraph = True)
    return os.linesep.join(result)

def get_text_chunks_from_text(text):
    return text_splitter.split_text(text)

def get_text_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features='html.parser')
    text = soup.get_text()
    lines = (line.strip() for line in text.splitlines())
    return os.linesep.join(line for line in lines if line)

def add_texts_to_store(texts, store):
    store.add_texts(texts)

def get_resources():
    return [p for p in glob('data/**/*', recursive=True) if os.path.isfile(p)]

def get_extension(file):
    return pathlib.Path(file).suffix

def another_run(query):
    resources = get_resources()
    store = get_store(STORE_FILE, None)
    for file in resources:
        print(f"Getting text from {file}")
        ext = get_extension(file)
        raw_text = ''
        if ext == '.md' or ext == '.csv':
            raw_text += get_text_from_file(file)
        elif ext == '.pdf':
            raw_text += get_text_from_pdf(file)
        elif ext == '.jpg' or ext == '.png':
            raw_text += get_text_from_image(file)
        else:
            print(f"Unrecognized extension: {ext}")
            continue
        texts = get_text_chunks_from_text(raw_text)
        if store is None:
            store = get_store(STORE_FILE, texts)
            continue
        add_texts_to_store(texts, store)
    docs = store.similarity_search(query)
    chain = load_qa_chain(ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo"), chain_type='stuff')
    return chain.run(input_documents=docs, question=query)

print(another_run(sys.argv[1]))

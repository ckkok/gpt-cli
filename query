#!/usr/bin/env python
from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.qa_with_sources import load_qa_with_sources_chain
from langchain.chains.question_answering import load_qa_chain
from langchain.chat_models import ChatOpenAI
from langchain.docstore.document import Document
from langchain.vectorstores import FAISS
import os
import json
import sys
import shutil
import requests
from bs4 import BeautifulSoup
from glob import glob
import easyocr
import cv2
import pathlib
import pypandoc
from dotenv import load_dotenv
load_dotenv()

STORE_FILE = 'store.faiss'
try:
    shutil.rmtree(STORE_FILE)
except:
    pass

text_splitter = CharacterTextSplitter(
        separator = "\n",
        chunk_size = 1000,
        chunk_overlap = 200,
        length_function = len,
)

def save_store(store):
    store.save_local(STORE_FILE)

def get_store(file, documents):
    if not os.path.exists(file):
        if documents is None:
            return None
        store = FAISS.from_documents(documents, OpenAIEmbeddings())
        save_store(store)
        return store
    return FAISS.load_local(STORE_FILE, OpenAIEmbeddings())

def get_documents_from_file(file):
    with open(file, 'r') as f:
        text = f.read()
        return [Document(page_content=x, metadata={"source": file}) for x in text_splitter.split_text(text)]

def get_documents_from_docx(file):
    metadata = {"source": file}
    text = pypandoc.convert_file(file, 'plain')
    return [Document(page_content=x, metadata=metadata) for x in text_splitter.split_text(text)]

def get_documents_from_pdf(file):
    reader = PdfReader(file)
    raw_text = ''
    for i, page in enumerate(reader.pages):
        text = page.extract_text()
        if text:
            raw_text += text
    return [Document(page_content=x, metadata={"source": file}) for x in text_splitter.split_text(raw_text)]

def get_documents_from_image(file):
    img = cv2.imread(file)
    reader = easyocr.Reader(['en'])
    result = reader.readtext(img, detail = 0, paragraph = True)
    text = os.linesep.join(result)
    return [Document(page_content=x, metadata={"source": file}) for x in text_splitter.split_text(text)]

def get_text_chunks_from_text(text):
    return text_splitter.split_text(text)

def get_documents_from_url(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features='html.parser')
    text = soup.get_text()
    lines = (line.strip() for line in text.splitlines())
    text = os.linesep.join(line for line in lines if line)
    return [Document(page_content=x, metadata={"source": url}) for x in text_splitter.split_text(text)]

def add_documents_to_store(documents, store):
    store.add_documents(documents)

def get_resources():
    return [p for p in glob('data/**/*', recursive=True) if os.path.isfile(p)]

def get_extension(file):
    return pathlib.Path(file).suffix

def get_web_sources():
    with open('./inventory.json', 'r') as f:
        return json.load(f)['web']

def main(query):
    resources = get_resources()
    web_resources = get_web_sources()
    store = get_store(STORE_FILE, None)
    for file in resources:
        print(f"Getting text from {file}")
        ext = get_extension(file)
        docs = None
        if ext == '.md' or ext == '.csv':
            docs = get_documents_from_file(file)
        elif ext == '.pdf':
            docs = get_documents_from_pdf(file)
        elif ext == '.docx':
            docs = get_documents_from_docx(file)
        elif ext == '.jpg' or ext == '.png':
            docs = get_documents_from_image(file)
        else:
            print(f"Unrecognized extension: {ext}")
            continue
        if store is None:
            store = get_store(STORE_FILE, docs)
            continue
        add_documents_to_store(docs, store)
    for web_source in web_resources:
        print(f"Getting webpage from {web_source}")
        docs = get_documents_from_url(web_source)
        if store is None:
            store = get_store(STORE_FILE, docs)
            continue
        add_documents_to_store(docs, store)
    save_store(store)
    docs = store.similarity_search(query)
    chain = load_qa_with_sources_chain(ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo"), chain_type='stuff')
    return chain.run(input_documents=docs, question=query)

print(main(sys.argv[1]))
